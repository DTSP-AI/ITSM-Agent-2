
BOTLY API DOCUMENTATION
========================

Base URL:
---------
https://api.getbotly.com

Main Endpoint:
--------------
POST /v1/chat/completions

Usage:
------
This endpoint is used to send a chat message and receive a response from the Botly AI model.

Example Request (Python - requests):
------------------------------------

import requests

response = requests.post(
    "https://api.getbotly.com/v1/chat/completions",
    headers={
        "Content-Type": "application/json",
        "Authorization": "Bearer YOUR_SECRET_TOKEN"
    },
    json={
        "model": "botly-v1-2025-04-09",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ],
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null,
        "presence_penalty": null,
        "stop": null,
        "max_tokens": null,
        "n": null,
        "stream": null
    }
)

Example Response:
-----------------

{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "botly-v1-2025-04-09",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello, how can I help you today?"
      },
      "finish_reason": "stop",
      "usage": {
        "prompt_tokens": 9,
        "completion_tokens": 12,
        "total_tokens": 21
      }
    }
  ]
}

Parameters:
-----------

model (string, required):
  - ID of the model to use. Must be "botly-v1-2025-04-09"

messages (array of objects, required):
  - Conversation history including system and user messages

temperature (float | null):
  - Sampling temperature (0-2). Higher = more random

top_p (float | null):
  - Nucleus sampling threshold (0-1). Alternative to temperature

frequency_penalty (float | null):
  - Penalizes new tokens based on frequency. Range: -2.0 to 2.0

presence_penalty (float | null):
  - Encourages topic diversity. Range: -2.0 to 2.0

stop (array | null):
  - Sequences where the generation will stop

max_tokens (int | null):
  - Max number of tokens in the response

n (int | null):
  - Number of responses to generate. Default is 1

stream (boolean | null):
  - Enables streaming responses like ChatGPT

Headers:
--------

Authorization: Bearer YOUR_SECRET_TOKEN
Content-Type: application/json

Notes:
------

- Always sanitize inputs to avoid injection or memory flooding
- Manage tokens wisely to reduce costs
- Use context windows to keep responses on track
